# -*- coding: utf-8 -*-
"""MScFE_690_Capstone - Group 5460 (Github File).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h0jZUu6sCWdB_XRZ4Ims_Ec_weht2pff

<br><br>
**Install Necessary Libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install matplotlib
# !pip install fredapi

"""<br><br>
**Importing Data and Conducting Exploratory Data Analysis**

<br><br>
**VARIABLE 1: TTF Natural Prices**

TTF Natural Gas Prices refer to the prices of natural gas traded at the Title Transfer Facility (TTF) in the Netherlands. TTF is one of the largest and most liquid natural gas trading hubs in Europe.
"""

import pandas as pd

ttf_price_url = 'https://raw.githubusercontent.com/yellowstickypine/WQU-690-Capstone-24-03/main/TTF%20Price%20History_raw.csv'
ttf_price_raw = pd.read_csv(ttf_price_url, delimiter=',')

print(ttf_price_raw)

# Convert 'Date' column to datetime format
ttf_price_raw['Date'] = pd.to_datetime(ttf_price_raw['Date'], format='%m/%d/%Y')

# Sort the DataFrame by 'Date' column in ascending order
ttf_price_raw.sort_values(by='Date', inplace=True)

# Reset index after sorting
ttf_price_raw.reset_index(drop=True, inplace=True)

# Print the updated DataFrame
print(ttf_price_raw)

# Adjust DataFrame to include only 'Date', 'Price', and 'Change%'
ttf_price_adjusted = ttf_price_raw[['Date', 'Price', 'Change %']]

# Print the adjusted DataFrame
print(ttf_price_adjusted)

import matplotlib.pyplot as plt

# Plot
plt.figure(figsize=(10, 6))
plt.plot(ttf_price_adjusted['Date'], ttf_price_adjusted['Price'], linestyle='-')

# Set labels and title
plt.xlabel('Date')
plt.ylabel('TTF Price (Eur/MMBtu)')
plt.title('TTF Price over Time')

# Rotate x-axis labels for better readability
plt.xticks(rotation=45)

# Show plot
plt.grid(True)
plt.tight_layout()
plt.show()

print("Summary Statistics of Price:")
print(ttf_price_adjusted['Price'].describe())

import matplotlib.pyplot as plt

# Plot
plt.figure(figsize=(10, 6))
plt.plot(ttf_price_adjusted['Date'], ttf_price_adjusted['Change %'].str.rstrip('%').astype(float), linestyle='-')

# Set labels and title
plt.xlabel('Date')
plt.ylabel('% Change in Daily TTF Price')
plt.title('TTF Price over Time')

# Rotate x-axis labels for better readability
plt.xticks(rotation=45)

# Show plot
plt.grid(True)
plt.tight_layout()
plt.show()

print("Summary Statistics of Price:")
print(ttf_price_adjusted['Change %'].str.rstrip('%').astype(float).describe())

import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm, t

# Convert 'Change %' column to string, remove '%' sign, and convert to numeric
ttf_price_adjusted['Change %'] = ttf_price_adjusted['Change %'].astype(str).apply(lambda x: float(x.strip('%')))

# Plot the histogram of 'Change %' column
plt.hist(ttf_price_adjusted['Change %'], bins=1000, density=True, alpha=0.6, color='g', label='Change % Histogram')

# Generate data points for the standard normal distribution
x = np.linspace(ttf_price_adjusted['Change %'].min(), ttf_price_adjusted['Change %'].max(), 1000)
standard_normal = norm.pdf(x, 0, 1)

# Generate data points for the standard t-distribution (with degrees of freedom = 30)
df = 5
standard_t = t.pdf(x, df)

# Plot the standard normal distribution
plt.plot(x, standard_normal, 'b-', label='Standard Normal Distribution')

# Plot the standard t-distribution
plt.plot(x, standard_t, 'r--', label=f'Standard t-distribution (df={df})')

# Set labels and title
plt.xlabel('Change %')
plt.ylabel('Density')
plt.title('Empirical Distribution vs. Standard Distributions')
plt.legend()

# Show plot
plt.grid(True)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm, t

# Convert 'Change %' column to string, remove '%' sign, and convert to numeric
ttf_price_adjusted['Change %'] = ttf_price_adjusted['Change %'].astype(str).apply(lambda x: float(x.strip('%')))

# Plot the histogram of 'Change %' column
plt.hist(ttf_price_adjusted['Change %'], bins=2000, density=True, alpha=0.6, color='g', label='Change % Histogram')

# Generate data points for the standard normal distribution within the range [-10, 10]
x_normal = np.linspace(-10, 10, 1000)
standard_normal = norm.pdf(x_normal, 0, 1)

# Plot the standard normal distribution
plt.plot(x_normal, standard_normal, 'b-', label='Standard Normal Distribution')

# Generate data points for the standard t-distribution (with degrees of freedom = 5) within the range [-10, 10]
standard_t = t.pdf(x_normal, 5)

# Plot the standard t-distribution
plt.plot(x_normal, standard_t, 'r--', label='Standard t-distribution (df=5)')

# Set x-axis limit
plt.xlim(-10, 10)

# Set labels and title
plt.xlabel('Change %')
plt.ylabel('Density')
plt.title('Empirical Distribution vs. Standard Distributions')
plt.legend()

# Show plot
plt.grid(True)
plt.tight_layout()
plt.show()

"""<br><br>
**VARIABLE 2: Natural Gas Storage in Europe**
"""

import pandas as pd

europegasstorage_url = 'https://raw.githubusercontent.com/yellowstickypine/WQU-690-Capstone-24-03/main/GasStorage_raw.csv'
europegasstorage_github_raw = pd.read_csv(europegasstorage_url, delimiter=';')

print(europegasstorage_github_raw)

# Rename 'Gas Day Start' column to 'Date'
europegasstorage_github_raw.rename(columns={'Gas Day Start': 'Date'}, inplace=True)

# Convert 'Date' column to datetime format
europegasstorage_github_raw['Date'] = pd.to_datetime(europegasstorage_github_raw['Date'], format='%Y-%m-%d')

# Sort the DataFrame by 'Date' column in ascending order
europegasstorage_github_raw.sort_values(by='Date', inplace=True)

# Reset index after sorting
europegasstorage_github_raw.reset_index(drop=True, inplace=True)

print(europegasstorage_github_raw)

# Adjust DataFrame to include only 'Date', and necessary columns
europegasstorage_adjusted = europegasstorage_github_raw[['Date', 'Gas in storage (TWh)', 'Full (%)', 'Trend (%)']]

europegasstorage_adjusted = europegasstorage_adjusted.rename(columns={'Gas in storage (TWh)': 'Gas Inventory TWh',
                                                               'Full (%)': 'Gas Inventory Full %',
                                                               'Trend (%)': 'Gas Inventory Trend %'})

# Print the adjusted DataFrame
print(europegasstorage_adjusted)

# Plotting each column separately
columns_to_plot = ["Gas Inventory TWh", "Gas Inventory Full %", "Gas Inventory Trend %"]
for column in columns_to_plot:
    plt.figure(figsize=(10, 6))
    plt.plot(europegasstorage_adjusted['Date'], europegasstorage_adjusted[column])
    plt.title(column)
    plt.xlabel('Date')
    plt.ylabel(column)
    plt.grid(True)
    plt.show()

"""<br><br>
**VARIABLE 3: LNG Storage in Europe**
"""

import pandas as pd

lngstorage_url = 'https://raw.githubusercontent.com/yellowstickypine/WQU-690-Capstone-24-03/main/LNGStorage_raw.csv'
lngstorage_github_raw = pd.read_csv(lngstorage_url, delimiter=';')

print(lngstorage_github_raw)

# Rename 'Gas Day Start' column to 'Date'
lngstorage_github_raw.rename(columns={'Gas Day Start': 'Date'}, inplace=True)

# Convert 'Date' column to datetime format
lngstorage_github_raw['Date'] = pd.to_datetime(lngstorage_github_raw['Date'], format='%Y-%m-%d')

# Sort the DataFrame by 'Date' column in ascending order
lngstorage_github_raw.sort_values(by='Date', inplace=True)

# Reset index after sorting
lngstorage_github_raw.reset_index(drop=True, inplace=True)

print(lngstorage_github_raw)

# Adjust DataFrame to include only 'Date', and necessary columns
lngstorage_adjusted = lngstorage_github_raw[['Date', 'LNG Inventory (10³ ㎥ LNG)']]

lngstorage_adjusted = lngstorage_adjusted.rename(columns={'LNG Inventory (10³ ㎥ LNG)': 'LNG Inventory m3'})

# Print the adjusted DataFrame
print(lngstorage_adjusted)

# Plotting each column separately
columns_to_plot = ["LNG Inventory m3"]
for column in columns_to_plot:
    plt.figure(figsize=(10, 6))
    plt.plot(lngstorage_adjusted['Date'], lngstorage_adjusted[column])
    plt.title(column)
    plt.xlabel('Date')
    plt.ylabel(column)
    plt.grid(True)
    plt.show()

"""<br><br>
**VARIABLE 4: Interest Rates in Europe (ECBDFR)**
"""

from fredapi import Fred

# Replace 'your_api_key' with your actual FRED API key
fred = Fred(api_key='0ecabe2a48130d6891c15fbc1b599d1c')

# Example: Downloading ECBDFR data
ECBDFR_raw = fred.get_series('ECBDFR', download=True)

ECBDFR_adjusted = ECBDFR_raw.to_frame(name='ECBDFR')
ECBDFR_adjusted.index.name = 'Date'
ECBDFR_adjusted = ECBDFR_adjusted.reset_index()
ECBDFR_adjusted['Date'] = pd.to_datetime(ECBDFR_adjusted['Date'], format='%Y-%m-%d')

print(ECBDFR_adjusted)

# Plotting each column separately
columns_to_plot = ["ECBDFR"]
for column in columns_to_plot:
    plt.figure(figsize=(10, 6))
    plt.plot(ECBDFR_adjusted['Date'], ECBDFR_adjusted[column])
    plt.title(column)
    plt.xlabel('Date')
    plt.ylabel(column)
    plt.grid(True)
    plt.show()

"""<br><br>
**VARIABLE 5: EUR-USD Exchange Rate (DEXUSEU)**
"""

from fredapi import Fred

# Replace 'your_api_key' with your actual FRED API key
fred = Fred(api_key='0ecabe2a48130d6891c15fbc1b599d1c')

# Example: Downloading ECBDFR data
DEXUSEU_raw = fred.get_series('DEXUSEU', download=True)

DEXUSEU_adjusted = DEXUSEU_raw.to_frame(name='DEXUSEU')
DEXUSEU_adjusted.index.name = 'Date'
DEXUSEU_adjusted = DEXUSEU_adjusted.reset_index()
DEXUSEU_adjusted['Date'] = pd.to_datetime(DEXUSEU_adjusted['Date'], format='%Y-%m-%d')

print(DEXUSEU_adjusted)

# Plotting each column separately
columns_to_plot = ["DEXUSEU"]
for column in columns_to_plot:
    plt.figure(figsize=(10, 6))
    plt.plot(DEXUSEU_adjusted['Date'], DEXUSEU_adjusted[column])
    plt.title(column)
    plt.xlabel('Date')
    plt.ylabel(column)
    plt.grid(True)
    plt.show()

"""<br><br>
**VARIABLE 6: Competing Fuel - Crude Oil Prices (DCOILBRENTEU)**
"""

from fredapi import Fred

# Replace 'your_api_key' with your actual FRED API key
fred = Fred(api_key='0ecabe2a48130d6891c15fbc1b599d1c')

# Example: Downloading ECBDFR data
DCOILBRENTEU_raw = fred.get_series('DCOILBRENTEU', download=True)

DCOILBRENTEU_adjusted = DCOILBRENTEU_raw.to_frame(name='DCOILBRENTEU')
DCOILBRENTEU_adjusted.index.name = 'Date'
DCOILBRENTEU_adjusted = DCOILBRENTEU_adjusted.reset_index()
DCOILBRENTEU_adjusted['Date'] = pd.to_datetime(DCOILBRENTEU_adjusted['Date'], format='%Y-%m-%d')

print(DCOILBRENTEU_adjusted)

# Plotting each column separately
columns_to_plot = ["DCOILBRENTEU"]
for column in columns_to_plot:
    plt.figure(figsize=(10, 6))
    plt.plot(DCOILBRENTEU_adjusted['Date'], DCOILBRENTEU_adjusted[column])
    plt.title(column)
    plt.xlabel('Date')
    plt.ylabel(column)
    plt.grid(True)
    plt.show()

"""<br><br>
**VARIABLE 7: Competing Fuel - Heating Oil Prices (DHOILNYH)**
"""

from fredapi import Fred

# Replace 'your_api_key' with your actual FRED API key
fred = Fred(api_key='0ecabe2a48130d6891c15fbc1b599d1c')

# Example: Downloading ECBDFR data
DHOILNYH_raw = fred.get_series('DHOILNYH', download=True)

DHOILNYH_adjusted = DHOILNYH_raw.to_frame(name='DHOILNYH')
DHOILNYH_adjusted.index.name = 'Date'
DHOILNYH_adjusted = DHOILNYH_adjusted.reset_index()
DHOILNYH_adjusted['Date'] = pd.to_datetime(DHOILNYH_adjusted['Date'], format='%Y-%m-%d')

print(DHOILNYH_adjusted)

# Plotting each column separately
columns_to_plot = ["DHOILNYH"]
for column in columns_to_plot:
    plt.figure(figsize=(10, 6))
    plt.plot(DHOILNYH_adjusted['Date'], DHOILNYH_adjusted[column])
    plt.title(column)
    plt.xlabel('Date')
    plt.ylabel(column)
    plt.grid(True)
    plt.show()

"""<br><br>
**VARIABLE 8 - Competing Fuel - USA Natural Gas Prices (DHHNGSP)**
"""

from fredapi import Fred

# Replace 'your_api_key' with your actual FRED API key
fred = Fred(api_key='0ecabe2a48130d6891c15fbc1b599d1c')

# Example: Downloading ECBDFR data
DHHNGSP_raw = fred.get_series('DHHNGSP', download=True)

DHHNGSP_adjusted = DHHNGSP_raw.to_frame(name='DHHNGSP')
DHHNGSP_adjusted.index.name = 'Date'
DHHNGSP_adjusted = DHHNGSP_adjusted.reset_index()
DHHNGSP_adjusted['Date'] = pd.to_datetime(DHHNGSP_adjusted['Date'], format='%Y-%m-%d')

print(DHHNGSP_adjusted)

# Plotting each column separately
columns_to_plot = ["DHHNGSP"]
for column in columns_to_plot:
    plt.figure(figsize=(10, 6))
    plt.plot(DHHNGSP_adjusted['Date'], DHHNGSP_adjusted[column])
    plt.title(column)
    plt.xlabel('Date')
    plt.ylabel(column)
    plt.grid(True)
    plt.show()

"""<br><br>
**VARIABLE 9-11: WEATHER CONDITIONS IN BERLIN, GERMANY (Temperature, Sunlight, Precipitation)**
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# !pip install openmeteo-requests
# !pip install requests-cache retry-requests numpy pandas

import openmeteo_requests

import requests_cache
import pandas as pd
from retry_requests import retry

# Setup the Open-Meteo API client with cache and retry on error
cache_session = requests_cache.CachedSession('.cache', expire_after = -1)
retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)
openmeteo = openmeteo_requests.Client(session = retry_session)

# Make sure all required weather variables are listed here
# The order of variables in hourly or daily is important to assign them correctly below
url = "https://archive-api.open-meteo.com/v1/archive"
params = {
	"latitude": 51.5,
	"longitude": 10.5,
	"start_date": "2011-01-01",
	"end_date": "2024-04-16",
	"daily": ["temperature_2m_mean", "apparent_temperature_mean", "daylight_duration", "sunshine_duration", "precipitation_sum"]
}
responses = openmeteo.weather_api(url, params=params)

# Process first location. Add a for-loop for multiple locations or weather models
response = responses[0]
print(f"Coordinates {response.Latitude()}°N {response.Longitude()}°E")
print(f"Elevation {response.Elevation()} m asl")
print(f"Timezone {response.Timezone()} {response.TimezoneAbbreviation()}")
print(f"Timezone difference to GMT+0 {response.UtcOffsetSeconds()} s")

# Process daily data. The order of variables needs to be the same as requested.
daily = response.Daily()
daily_temperature_2m_mean = daily.Variables(0).ValuesAsNumpy()
daily_apparent_temperature_mean = daily.Variables(1).ValuesAsNumpy()
daily_daylight_duration = daily.Variables(2).ValuesAsNumpy()
daily_sunshine_duration = daily.Variables(3).ValuesAsNumpy()
daily_precipitation_sum = daily.Variables(4).ValuesAsNumpy()

daily_data = {"date": pd.date_range(
	start = pd.to_datetime(daily.Time(), unit = "s", utc = True),
	end = pd.to_datetime(daily.TimeEnd(), unit = "s", utc = True),
	freq = pd.Timedelta(seconds = daily.Interval()),
	inclusive = "left"
)}
daily_data["temperature_2m_mean"] = daily_temperature_2m_mean
daily_data["apparent_temperature_mean"] = daily_apparent_temperature_mean
daily_data["daylight_duration"] = daily_daylight_duration
daily_data["sunshine_duration"] = daily_sunshine_duration
daily_data["precipitation_sum"] = daily_precipitation_sum

berlin_weather_raw = pd.DataFrame(data = daily_data)
print(berlin_weather_raw)

# Rename 'Gas Day Start' column to 'Date'
berlin_weather_raw.rename(columns={'date': 'Date'}, inplace=True)

# Convert 'Date' column to datetime format
berlin_weather_raw['Date'] = pd.to_datetime(berlin_weather_raw['Date']).dt.date
berlin_weather_raw['Date'] = pd.to_datetime(berlin_weather_raw['Date'], format='%Y-%m-%d')

# Sort the DataFrame by 'Date' column in ascending order
berlin_weather_raw.sort_values(by='Date', inplace=True)

# Reset index after sorting
berlin_weather_raw.reset_index(drop=True, inplace=True)

print(berlin_weather_raw)

# Adjust DataFrame to include only 'Date', and necessary columns
berlin_weather_adjusted = berlin_weather_raw[['Date', 'apparent_temperature_mean', 'sunshine_duration', 'precipitation_sum']]

berlin_weather_adjusted = berlin_weather_adjusted.rename(columns={'apparent_temperature_mean': 'Berlin Temperature',
                                                                  'sunshine_duration': 'Berlin Sunshine',
                                                                  'precipitation_sum': 'Berlin Rainfall'})

# Print the adjusted DataFrame
print(berlin_weather_adjusted)

# Plotting each column separately
columns_to_plot = ["Berlin Temperature", 'Berlin Sunshine', 'Berlin Rainfall']
for column in columns_to_plot:
    plt.figure(figsize=(10, 6))
    plt.plot(berlin_weather_adjusted['Date'], berlin_weather_adjusted[column])
    plt.title(column)
    plt.xlabel('Date')
    plt.ylabel(column)
    plt.grid(True)
    plt.show()

"""<br><br>
**VARIABLE 12-14: WEATHER CONDITIONS IN LONDON, UNITED KINGDOM (Temperature, Sunlight, Precipitation)**
"""

import openmeteo_requests

import requests_cache
import pandas as pd
from retry_requests import retry

# Setup the Open-Meteo API client with cache and retry on error
cache_session = requests_cache.CachedSession('.cache', expire_after = -1)
retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)
openmeteo = openmeteo_requests.Client(session = retry_session)

# Make sure all required weather variables are listed here
# The order of variables in hourly or daily is important to assign them correctly below
url = "https://archive-api.open-meteo.com/v1/archive"
params = {
	"latitude": 51.4,
	"longitude": -0.1,
	"start_date": "2011-01-01",
	"end_date": "2024-04-16",
	"daily": ["temperature_2m_mean", "apparent_temperature_mean", "daylight_duration", "sunshine_duration", "precipitation_sum"]
}
responses = openmeteo.weather_api(url, params=params)

# Process first location. Add a for-loop for multiple locations or weather models
response = responses[0]
print(f"Coordinates {response.Latitude()}°N {response.Longitude()}°E")
print(f"Elevation {response.Elevation()} m asl")
print(f"Timezone {response.Timezone()} {response.TimezoneAbbreviation()}")
print(f"Timezone difference to GMT+0 {response.UtcOffsetSeconds()} s")

# Process daily data. The order of variables needs to be the same as requested.
daily = response.Daily()
daily_temperature_2m_mean = daily.Variables(0).ValuesAsNumpy()
daily_apparent_temperature_mean = daily.Variables(1).ValuesAsNumpy()
daily_daylight_duration = daily.Variables(2).ValuesAsNumpy()
daily_sunshine_duration = daily.Variables(3).ValuesAsNumpy()
daily_precipitation_sum = daily.Variables(4).ValuesAsNumpy()

daily_data = {"date": pd.date_range(
	start = pd.to_datetime(daily.Time(), unit = "s", utc = True),
	end = pd.to_datetime(daily.TimeEnd(), unit = "s", utc = True),
	freq = pd.Timedelta(seconds = daily.Interval()),
	inclusive = "left"
)}
daily_data["temperature_2m_mean"] = daily_temperature_2m_mean
daily_data["apparent_temperature_mean"] = daily_apparent_temperature_mean
daily_data["daylight_duration"] = daily_daylight_duration
daily_data["sunshine_duration"] = daily_sunshine_duration
daily_data["precipitation_sum"] = daily_precipitation_sum

london_weather_raw = pd.DataFrame(data = daily_data)
print(london_weather_raw)

# Rename 'Gas Day Start' column to 'Date'
london_weather_raw.rename(columns={'date': 'Date'}, inplace=True)

# Convert 'Date' column to datetime format
london_weather_raw['Date'] = pd.to_datetime(london_weather_raw['Date']).dt.date
london_weather_raw['Date'] = pd.to_datetime(london_weather_raw['Date'], format='%Y-%m-%d')

# Sort the DataFrame by 'Date' column in ascending order
london_weather_raw.sort_values(by='Date', inplace=True)

# Reset index after sorting
london_weather_raw.reset_index(drop=True, inplace=True)

print(london_weather_raw)

# Adjust DataFrame to include only 'Date', and necessary columns
london_weather_adjusted = london_weather_raw[['Date', 'apparent_temperature_mean', 'sunshine_duration', 'precipitation_sum']]

london_weather_adjusted = london_weather_adjusted.rename(columns={'apparent_temperature_mean': 'London Temperature',
                                                                  'sunshine_duration': 'London Sunshine',
                                                                  'precipitation_sum': 'London Rainfall'})

# Print the adjusted DataFrame
print(london_weather_adjusted)

# Plotting each column separately
columns_to_plot = ["London Temperature", 'London Sunshine', 'London Rainfall']
for column in columns_to_plot:
    plt.figure(figsize=(10, 6))
    plt.plot(london_weather_adjusted['Date'], london_weather_adjusted[column])
    plt.title(column)
    plt.xlabel('Date')
    plt.ylabel(column)
    plt.grid(True)
    plt.show()

"""<br><br>
**DATA PREPROCESSING AND MERGING VARIABLES FOR FURTHER ANALYSIS**

"""

import pandas as pd

# List of DataFrames
list_of_variables = [ttf_price_adjusted, europegasstorage_adjusted,
                     lngstorage_adjusted, ECBDFR_adjusted,
                     DEXUSEU_adjusted, DCOILBRENTEU_adjusted,
                     DHOILNYH_adjusted, DHHNGSP_adjusted,
                     berlin_weather_adjusted, london_weather_adjusted]

# Iterative Merging (optional)
varibles_merged_raw = ttf_price_adjusted.merge(right=europegasstorage_adjusted, how='outer', on='Date')
for variables in list_of_variables[2:]:
  varibles_merged_raw = varibles_merged_raw.merge(right=variables, how='outer', on='Date')

# Drop rows with any NA value (default behavior)
varibles_merged_drop = varibles_merged_raw.dropna()

print(varibles_merged_drop)

"""<br> <br>
**Modelling a Bayesian Network**




"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install pgmpy

print(varibles_merged_drop)

import pandas as pd
from pgmpy.estimators import PC
from pgmpy.models import BayesianModel

bayesian_network_dataframe = varibles_merged_drop.copy()

print(bayesian_network_dataframe)

import pandas as pd
from pgmpy.estimators import HillClimbSearch, BDeuScore, K2Score
from pgmpy.models import BayesianNetwork
import matplotlib.pyplot as plt
import networkx as nx

print("Data shape:", bayesian_network_dataframe.shape)
print("Missing values:")
print(bayesian_network_dataframe.isnull().sum())

column_names = list(bayesian_network_dataframe)
print(column_names)

from sklearn.preprocessing import MinMaxScaler

# Select relevant columns for analysis
columns_of_interest = ["Price", "Change %", "Gas Inventory TWh",
                       "Gas Inventory Full %", 'Gas Inventory Trend %', 'LNG Inventory m3',
                       'ECBDFR', 'DEXUSEU', 'DCOILBRENTEU',
                       'DHOILNYH', 'DHHNGSP', 'Berlin Temperature',
                       'Berlin Sunshine', 'Berlin Rainfall', 'London Temperature',
                       'London Sunshine', 'London Rainfall']

data = bayesian_network_dataframe[columns_of_interest]

# Initialize MinMaxScaler
scaler = MinMaxScaler()

# Normalize the selected columns
data_normalized = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)

# Display the normalized DataFrame
print(data_normalized)

"""<br><br>
**VISUALISATION OF CORRELATION MATRIX**
"""

import seaborn as sns

# Calculate the correlation matrix
correlation_matrix = data_normalized.corr()

# Visualize the correlation matrix
plt.figure(figsize=(10, 6))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Matrix")
plt.show()

"""<br><br>
**CONDUCTING HILL CLIMB SEARCH AND FEATURE SELECTION**
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Create a HillClimbSearch object
# hc = HillClimbSearch(data_normalized)
# 
# # Track scores over iterations
# scores = []
# 
# # Set maximum number of iterations
# max_iterations = 10
# 
# # Iterate
# for i in range(max_iterations):
#     # Search for the best DAG structure using BDeuScore as scoring method
#     model = hc.estimate(scoring_method=BDeuScore(data_normalized))
# 
#     # Compute score
#     score = BDeuScore(data_normalized).score(model)
# 
#     # Append score
#     scores.append(score)
# 
#     # Check for convergence
#     if i > 0 and abs(scores[-1] - scores[-2]) < 1e-1:
#         print("Convergence reached")
#         break
#

print("Bayesian Network Structure:")
print(model.edges())

"""<br><br>
**VISUALISATION OF DAG**
"""

import matplotlib.pyplot as plt
import networkx as nx

# Create a directed graph from the DAG structure
dag = nx.DiGraph(model.edges())

# Plot the directed graph
plt.figure(figsize=(20, 10))
pos = nx.spring_layout(dag)  # Positions nodes using Fruchterman-Reingold force-directed algorithm
nx.draw(dag, pos, with_labels=True, node_size=2000, node_color="skyblue", font_size=10, font_weight="bold", arrowsize=30)
plt.title("Directed Acyclic Graph (DAG)")
plt.show()

"""<br><br>
**ADDITION OF TEST-TRAIN SPLIT AND FEATURE ELIMINATION**

"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestRegressor
from pgmpy.estimators import HillClimbSearch, BDeuScore
import matplotlib.pyplot as plt
import networkx as nx

# Select relevant columns for analysis
columns_of_interest = ["Price", "Change %", "Gas Inventory TWh",
                       "Gas Inventory Full %", 'Gas Inventory Trend %', 'LNG Inventory m3',
                       'ECBDFR', 'DEXUSEU', 'DCOILBRENTEU',
                       'DHOILNYH', 'DHHNGSP', 'Berlin Temperature',
                       'Berlin Sunshine', 'Berlin Rainfall', 'London Temperature',
                       'London Sunshine', 'London Rainfall']

data = bayesian_network_dataframe[columns_of_interest]

# Initialize MinMaxScaler
scaler = MinMaxScaler()

# Normalize the selected columns
new_data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)

# Split the data into training and testing sets
train_data, test_data = train_test_split(new_data, test_size=0.3, random_state=42)

# Calculate feature importances using RandomForestRegressor
reg = RandomForestRegressor()
reg.fit(train_data.drop("Change %", axis=1), train_data["Change %"])
importance_scores = reg.feature_importances_

# Define a threshold for feature importance
threshold = 0.05  # Example threshold value (adjust as needed)

# Create a boolean mask to identify important features based on the threshold
important_features_mask = importance_scores >= threshold

# Get the column names corresponding to important features
important_features = train_data.drop("Change %", axis=1).columns[important_features_mask]

# Filter the data to retain only important features
train_data_filtered = train_data[important_features]
test_data_filtered = test_data[important_features]

# Create a HillClimbSearch object with the filtered training data
hc = HillClimbSearch(train_data_filtered)

# Track scores over iterations
scores = []

# Set maximum number of iterations
max_iterations = 10

# Iterate
for i in range(max_iterations):
    # Search for the best DAG structure using BDeuScore as scoring method
    learned_model = hc.estimate(scoring_method=BDeuScore(train_data_filtered))

    # Compute score on test data for evaluation
    test_score = BDeuScore(test_data_filtered).score(learned_model)
    scores.append(test_score)

    # Check for convergence
    if i > 0 and abs(scores[-1] - scores[-2]) < 1e-8:
        print("Convergence reached")
        break

# Print Bayesian Network Structure learned from filtered training data
print("Bayesian Network Structure:")
print(learned_model.edges())

"""<br><br>
**VISUALISATION OF DAG**
"""

# Create a directed graph from the DAG structure
dag = nx.DiGraph(learned_model.edges())

# Define the node positions manually (adjust as needed)
pos = nx.spring_layout(dag, scale=1)

# Set the center node ('Price') position at the center of the plot
pos["Price"] = [0.1, 0.1]  # Place the "Price" node at the center (adjust coordinates as needed)

# Plot the directed graph with adjusted node positions
plt.figure(figsize=(20, 10))
nx.draw(dag, pos, with_labels=True, node_size=2000, node_color="skyblue", font_size=10, font_weight="bold", arrowsize=30)
plt.title("Directed Acyclic Graph (DAG) learned from Filtered Training Data")
plt.show()

"""References:

https://www.ecad.eu/dailydata/index.php

https://confluence.ecmwf.int/display/DAC/ECMWF+open+data%3A+real-time+forecasts+from+IFS+and+AIFS

https://www.dwd.de/EN/climate_environment/cdc/cdc_node_en.html

https://open-meteo.com/en/docs/historical-weather-api#latitude=51.5&longitude=10.5&start_date=2023-01-01&end_date=2024-01-01&hourly=&daily=temperature_2m_mean
"""
